Questions

    a. Why are we regularizing with respect to sparsity?
    
When there's less data, it's easier to overfit the process. By increasing the amount of weighted zeroes, the process is sped up and helps generalize the algorithm.

    b. How does L1 regularization increase sparsity?

L1 underfits. Not all features are useful or even contributing to a correct fitted model model, so removing the less useful ones can be advantageous.

    c. Task 1: Here, just report the best log loss value / model size you can get and what gamma value you used to get them.

I got a .25 LogLoss with .7 regularization strength